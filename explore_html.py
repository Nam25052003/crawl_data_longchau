#!/usr/bin/env python3
"""
Script kh√°m ph√° c·∫•u tr√∫c HTML c·ªßa Long Ch√¢u ƒë·ªÉ t√¨m CSS selectors ƒë√∫ng
"""
import requests
from bs4 import BeautifulSoup
import sys
import os

# Th√™m th∆∞ m·ª•c g·ªëc v√†o Python path
sys.path.append(os.path.dirname(__file__))

from config.settings import BASE_URL, DEFAULT_HEADERS

def explore_page_structure(url):
    """Kh√°m ph√° c·∫•u tr√∫c HTML c·ªßa m·ªôt trang"""
    print(f"üîç Exploring: {url}")
    
    try:
        response = requests.get(url, headers=DEFAULT_HEADERS, timeout=10)
        soup = BeautifulSoup(response.content, 'html.parser')
        
        print(f"‚úÖ Status: {response.status_code}")
        print(f"üìÑ Title: {soup.title.string if soup.title else 'No title'}")
        
        # T√¨m t·∫•t c·∫£ c√°c link c√≥ ch·ª©a t·ª´ kh√≥a s·∫£n ph·∫©m
        product_keywords = ['san-pham', 'product', 'thuoc', 'item']
        
        print("\nüì¶ Looking for product links...")
        product_links = []
        
        for keyword in product_keywords:
            links = soup.find_all('a', href=lambda x: x and keyword in x.lower())
            if links:
                print(f"   Found {len(links)} links containing '{keyword}':")
                for i, link in enumerate(links[:5]):  # Ch·ªâ hi·ªÉn th·ªã 5 link ƒë·∫ßu
                    href = link.get('href')
                    text = link.get_text().strip()[:50]
                    if href.startswith('/'):
                        href = BASE_URL + href
                    print(f"     {i+1}. {text} -> {href}")
                    product_links.append(href)
                if len(links) > 5:
                    print(f"     ... and {len(links) - 5} more")
        
        # T√¨m c√°c class c√≥ th·ªÉ ch·ª©a s·∫£n ph·∫©m
        print("\nüéØ Looking for product containers...")
        potential_selectors = [
            '[class*="product"]',
            '[class*="item"]', 
            '[class*="card"]',
            '[class*="box"]',
            '.row > div',
            '.grid > div'
        ]
        
        for selector in potential_selectors:
            elements = soup.select(selector)
            if elements:
                print(f"   {selector}: {len(elements)} elements")
                # Hi·ªÉn th·ªã class c·ªßa element ƒë·∫ßu ti√™n
                if elements[0].get('class'):
                    print(f"     Example classes: {' '.join(elements[0].get('class'))}")
        
        # T√¨m c√°c th·∫ª img (c√≥ th·ªÉ l√† ·∫£nh s·∫£n ph·∫©m)
        print("\nüñºÔ∏è  Looking for images...")
        images = soup.find_all('img')
        product_images = [img for img in images if img.get('src') and any(keyword in img.get('src', '').lower() for keyword in ['product', 'thuoc', 'item'])]
        
        if product_images:
            print(f"   Found {len(product_images)} potential product images")
            for i, img in enumerate(product_images[:3]):
                src = img.get('src')
                alt = img.get('alt', 'No alt')[:30]
                print(f"     {i+1}. {alt} -> {src}")
        
        # L∆∞u HTML ƒë·ªÉ debug
        with open('debug_page.html', 'w', encoding='utf-8') as f:
            f.write(str(soup.prettify()))
        print(f"\nüíæ Saved HTML to debug_page.html for manual inspection")
        
        return product_links[:5]  # Tr·∫£ v·ªÅ 5 link s·∫£n ph·∫©m ƒë·∫ßu ti√™n
        
    except Exception as e:
        print(f"‚ùå Error: {e}")
        return []

def test_product_page(product_url):
    """Test c·∫•u tr√∫c trang s·∫£n ph·∫©m"""
    print(f"\nüîç Testing product page: {product_url}")
    
    try:
        response = requests.get(product_url, headers=DEFAULT_HEADERS, timeout=10)
        soup = BeautifulSoup(response.content, 'html.parser')
        
        print(f"‚úÖ Status: {response.status_code}")
        
        # T√¨m c√°c element c√≥ th·ªÉ ch·ª©a th√¥ng tin s·∫£n ph·∫©m
        info_selectors = {
            'title': ['h1', '.title', '.product-title', '.name'],
            'price': ['.price', '.cost', '.gia', '[class*="price"]'],
            'description': ['.description', '.desc', '.mo-ta', '.content'],
            'image': ['img[src*="product"]', '.product-image img', '.main-image img']
        }
        
        for info_type, selectors in info_selectors.items():
            print(f"\nüìã Looking for {info_type}...")
            for selector in selectors:
                elements = soup.select(selector)
                if elements:
                    element = elements[0]
                    if info_type == 'image':
                        content = element.get('src', 'No src')
                    else:
                        content = element.get_text().strip()[:100]
                    print(f"   ‚úÖ {selector}: {content}")
                    break
            else:
                print(f"   ‚ùå No {info_type} found")
        
    except Exception as e:
        print(f"‚ùå Error: {e}")

if __name__ == "__main__":
    # Test trang danh m·ª•c
    category_url = f"{BASE_URL}/thuoc"
    product_links = explore_page_structure(category_url)
    
    # Test trang s·∫£n ph·∫©m n·∫øu t√¨m th·∫•y
    if product_links:
        test_product_page(product_links[0])
    else:
        print("\n‚ö†Ô∏è  No product links found to test")
        
        # Th·ª≠ test trang ch·ªß
        print("\nüè† Testing homepage instead...")
        homepage_links = explore_page_structure(BASE_URL)
        if homepage_links:
            test_product_page(homepage_links[0])
