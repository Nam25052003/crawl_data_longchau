#!/usr/bin/env python3
"""
Script ch√≠nh ƒë·ªÉ ch·∫°y crawler Long Ch√¢u
"""
import argparse
import sys
import os

# Th√™m th∆∞ m·ª•c g·ªëc v√†o Python path
sys.path.append(os.path.dirname(__file__))

from src.crawlers.longchau_crawler import LongChauCrawler
from config.settings import *

def main():
    parser = argparse.ArgumentParser(description='Crawler cho website Long Ch√¢u')
    
    # C√°c t√πy ch·ªçn crawl
    parser.add_argument('--category-url', '-u', 
                       type=str,
                       help='URL danh m·ª•c c·ª• th·ªÉ (VD: thuc-pham-chuc-nang/canxi-vitamin-D)')
    parser.add_argument('--mode', '-m',
                       choices=['single', 'vitamin', 'subcategory', 'all'],
                       default='single',
                       help='Ch·∫ø ƒë·ªô crawl (m·∫∑c ƒë·ªãnh: single)')
    parser.add_argument('--main-category', 
                       type=str,
                       help='Danh m·ª•c ch√≠nh (VD: thuc-pham-chuc-nang)')
    parser.add_argument('--subcategories',
                       type=str, 
                       nargs='+',
                       help='Danh s√°ch subcategories')
    
    # C√°c t√πy ch·ªçn gi·ªõi h·∫°n
    parser.add_argument('--max-products', '-n', 
                       type=int, 
                       default=10,
                       help='S·ªë s·∫£n ph·∫©m t·ªëi ƒëa m·ªói danh m·ª•c (m·∫∑c ƒë·ªãnh: 10)')
    parser.add_argument('--max-products-per-category', 
                       type=int,
                       help='S·ªë s·∫£n ph·∫©m t·ªëi ƒëa m·ªói subcategory')
    
    # C√°c t√πy ch·ªçn output
    parser.add_argument('--output-format', '-f', 
                       choices=['json', 'csv', 'both'], 
                       default='both',
                       help='ƒê·ªãnh d·∫°ng file output (m·∫∑c ƒë·ªãnh: both)')
    parser.add_argument('--verbose', '-v', 
                       action='store_true',
                       help='Hi·ªÉn th·ªã log chi ti·∫øt')
    
    args = parser.parse_args()
    
    # T·∫°o th∆∞ m·ª•c c·∫ßn thi·∫øt
    os.makedirs('data', exist_ok=True)
    os.makedirs('logs', exist_ok=True)
    
    # Kh·ªüi t·∫°o crawler
    crawler = LongChauCrawler()
    
    try:
        # Reset categories tr∆∞·ªõc khi b·∫Øt ƒë·∫ßu crawl m·ªõi
        crawler.reset_categories()
        if args.mode == 'single':
            if not args.category_url:
                print("‚ùå C·∫ßn cung c·∫•p --category-url cho ch·∫ø ƒë·ªô single")
                print("VD: python main.py --mode single --category-url thuc-pham-chuc-nang/canxi-vitamin-D")
                return
            
            print(f"üöÄ Crawl danh m·ª•c: {args.category_url}")
            crawler.crawl_category(args.category_url, max_products=args.max_products)
            
        elif args.mode == 'vitamin':
            print("üöÄ Crawl t·∫•t c·∫£ danh m·ª•c vitamin v√† kho√°ng ch·∫•t...")
            crawler.crawl_vitamin_categories(max_products_per_category=args.max_products_per_category or args.max_products)
            
        elif args.mode == 'subcategory':
            if not args.main_category or not args.subcategories:
                print("‚ùå C·∫ßn cung c·∫•p --main-category v√† --subcategories cho ch·∫ø ƒë·ªô subcategory")
                print("VD: python main.py --mode subcategory --main-category thuc-pham-chuc-nang --subcategories canxi-vitamin-D vitamin-tong-hop")
                return
            
            print(f"üöÄ Crawl subcategories c·ªßa {args.main_category}...")
            crawler.crawl_subcategories(
                args.main_category, 
                args.subcategories,
                max_products_per_category=args.max_products_per_category or args.max_products
            )
            
        elif args.mode == 'all':
            print("üöÄ Crawl t·∫•t c·∫£ danh m·ª•c (ch∆∞a implement - s·ª≠ d·ª•ng mode kh√°c)")
            return
        
        # L∆∞u d·ªØ li·ªáu
        crawler.save_data(args.output_format)
        print("‚úÖ Ho√†n th√†nh crawler!")
        
    except KeyboardInterrupt:
        print("\n‚èπÔ∏è  ƒê√£ d·ª´ng crawler. ƒêang l∆∞u d·ªØ li·ªáu hi·ªán c√≥...")
        crawler.save_data(args.output_format)
        print("üíæ ƒê√£ l∆∞u d·ªØ li·ªáu.")
    except Exception as e:
        print(f"‚ùå L·ªói: {str(e)}")
        crawler.save_data(args.output_format)
        sys.exit(1)

if __name__ == "__main__":
    main()
