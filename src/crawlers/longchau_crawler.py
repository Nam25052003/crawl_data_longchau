"""
Crawler ch√≠nh cho website Long Ch√¢u
"""
import sys
import os
sys.path.append(os.path.dirname(os.path.dirname(os.path.dirname(__file__))))

import requests
from bs4 import BeautifulSoup
from typing import List, Dict, Any
import time
from tqdm import tqdm
import logging
from datetime import datetime
from selenium import webdriver
from selenium.webdriver.common.by import By
from selenium.webdriver.support.ui import WebDriverWait
from selenium.webdriver.support import expected_conditions as EC
from selenium.webdriver.chrome.options import Options
from selenium.webdriver.chrome.service import Service
from selenium.common.exceptions import TimeoutException, NoSuchElementException
from webdriver_manager.chrome import ChromeDriverManager

from config.settings import *
from src.utils.helpers import *

class LongChauCrawler:
    def __init__(self):
        self.session = requests.Session()
        self.session.headers.update(DEFAULT_HEADERS)
        self.logger = setup_logging()
        self.products = []
        self.driver = None
        self.current_categories = []  # L∆∞u tr·ªØ danh s√°ch categories ƒë√£ crawl
    
    def __del__(self):
        """Destructor ƒë·ªÉ ƒë·∫£m b·∫£o Selenium driver ƒë∆∞·ª£c ƒë√≥ng"""
        self.close_selenium_driver()
    
    def init_selenium_driver(self):
        """Kh·ªüi t·∫°o Selenium WebDriver"""
        if self.driver is None:
            chrome_options = Options()
            chrome_options.add_argument('--headless')  # Ch·∫°y ·∫©n browser
            chrome_options.add_argument('--no-sandbox')
            chrome_options.add_argument('--disable-dev-shm-usage')
            chrome_options.add_argument('--disable-gpu')
            chrome_options.add_argument('--window-size=1920,1080')
            chrome_options.add_argument(f'--user-agent={USER_AGENT}')
            
            try:
                # S·ª≠ d·ª•ng webdriver-manager ƒë·ªÉ t·ª± ƒë·ªông t·∫£i ChromeDriver
                service = Service(ChromeDriverManager().install())
                self.driver = webdriver.Chrome(service=service, options=chrome_options)
                self.driver.implicitly_wait(10)
                self.logger.info("ƒê√£ kh·ªüi t·∫°o Selenium WebDriver")
            except Exception as e:
                self.logger.error(f"L·ªói kh·ªüi t·∫°o WebDriver: {str(e)}")
                raise
    
    def close_selenium_driver(self):
        """ƒê√≥ng Selenium WebDriver"""
        if self.driver:
            self.driver.quit()
            self.driver = None
            self.logger.info("ƒê√£ ƒë√≥ng Selenium WebDriver")
        
    def get_product_urls(self, category_url: str) -> List[str]:
        """L·∫•y danh s√°ch URL s·∫£n ph·∫©m t·ª´ m·ªôt danh m·ª•c v·ªõi x·ª≠ l√Ω n√∫t 'Xem th√™m'"""
        product_urls = []
        
        try:
            url = f"{BASE_URL}/{category_url}"
            self.logger.info(f"Crawling category page: {url}")
            
            # Kh·ªüi t·∫°o Selenium driver
            self.init_selenium_driver()
            self.driver.get(url)
            
            # ƒê·ª£i trang load
            WebDriverWait(self.driver, 10).until(
                EC.presence_of_element_located((By.TAG_NAME, "body"))
            )
            
            # Click v√†o n√∫t "Xem th√™m" cho ƒë·∫øn khi kh√¥ng c√≤n n√∫t n√†o
            max_clicks = 20  # Gi·ªõi h·∫°n s·ªë l·∫ßn click ƒë·ªÉ tr√°nh v√≤ng l·∫∑p v√¥ t·∫≠n
            click_count = 0
            
            while click_count < max_clicks:
                try:
                    # T√¨m n√∫t "Xem th√™m"
                    see_more_button = WebDriverWait(self.driver, 3).until(
                        EC.element_to_be_clickable((By.XPATH, "//button[contains(.//span, 'Xem th√™m') and contains(.//span, 's·∫£n ph·∫©m')]"))
                    )
                    
                    # Scroll ƒë·∫øn n√∫t
                    self.driver.execute_script("arguments[0].scrollIntoView(true);", see_more_button)
                    time.sleep(1)
                    
                    # Click v√†o n√∫t
                    see_more_button.click()
                    click_count += 1
                    
                    self.logger.info(f"ƒê√£ click 'Xem th√™m' l·∫ßn {click_count}")
                    
                    # ƒê·ª£i content load
                    time.sleep(2)
                    
                except TimeoutException:
                    # Kh√¥ng c√≤n n√∫t "Xem th√™m"
                    self.logger.info("Kh√¥ng c√≤n n√∫t 'Xem th√™m', ƒë√£ load t·∫•t c·∫£ s·∫£n ph·∫©m")
                    break
                except Exception as e:
                    self.logger.warning(f"L·ªói khi click 'Xem th√™m': {str(e)}")
                    break
            
            # L·∫•y HTML sau khi ƒë√£ load t·∫•t c·∫£ s·∫£n ph·∫©m
            page_source = self.driver.page_source
            soup = BeautifulSoup(page_source, 'html.parser')
            
            # T√¨m grid s·∫£n ph·∫©m ch√≠nh - grid c√≥ class 'grid-cols-2' v√† 'md:grid-cols-4'
            product_grid = None
            all_grids = soup.find_all('div', class_=True)
            
            for div in all_grids:
                classes = div.get('class', [])
                if (isinstance(classes, list) and 
                    'grid' in classes and 
                    'grid-cols-2' in classes and 
                    'md:grid-cols-4' in classes):
                    product_grid = div
                    break
            
            if product_grid:
                # L·∫•y t·∫•t c·∫£ product links t·ª´ grid ch√≠nh
                all_links_in_grid = product_grid.find_all('a', href=lambda x: x and '.html' in x)
                
                # L·ªçc ch·ªâ l·∫•y links trong category hi·ªán t·∫°i
                category_path = f"/{category_url.split('/')[0]}/"  # V√≠ d·ª•: /thuc-pham-chuc-nang/
                
                seen_urls = set()
                for link in all_links_in_grid:
                    href = link.get('href')
                    if (href and 
                        category_path in href and 
                        href.count('/') >= 2):  # ƒê·∫£m b·∫£o l√† URL s·∫£n ph·∫©m chi ti·∫øt
                        
                        if href.startswith('/'):
                            href = BASE_URL + href
                        
                        # Ch·ªâ th√™m n·∫øu ch∆∞a th·∫•y URL n√†y
                        if href not in seen_urls:
                            seen_urls.add(href)
                            product_urls.append(href)
                
                self.logger.info(f"T√¨m th·∫•y grid s·∫£n ph·∫©m ch√≠nh v·ªõi {len(product_urls)} s·∫£n ph·∫©m unique")
            else:
                # Fallback: l·∫•y t·∫•t c·∫£ unique links trong category
                self.logger.warning("Kh√¥ng t√¨m th·∫•y grid s·∫£n ph·∫©m ch√≠nh, s·ª≠ d·ª•ng fallback")
                
                category_path = f"/{category_url.split('/')[0]}/"
                all_links = soup.find_all('a', href=lambda x: x and '.html' in x and category_path in x)
                
                seen_urls = set()
                for link in all_links:
                    href = link.get('href')
                    if href and href.count('/') >= 2:
                        if href.startswith('/'):
                            href = BASE_URL + href
                        
                        if href not in seen_urls:
                            seen_urls.add(href)
                            product_urls.append(href)
                
                self.logger.info(f"Fallback: t√¨m th·∫•y {len(product_urls)} s·∫£n ph·∫©m unique")
            
        except Exception as e:
            self.logger.error(f"L·ªói khi crawl danh m·ª•c {category_url}: {str(e)}")
        finally:
            # ƒê√≥ng Selenium driver
            self.close_selenium_driver()
        
        # Filter ch·ªâ l·∫•y URL s·∫£n ph·∫©m th·ª±c s·ª± (ƒë√£ lo·∫°i b·ªè duplicate ·ªü tr√™n)
        filtered_urls = [url for url in product_urls if self.is_product_url(url)]
        
        self.logger.info(f"T√¨m th·∫•y {len(filtered_urls)} s·∫£n ph·∫©m trong danh m·ª•c {category_url} (sau khi load t·∫•t c·∫£)")
        return filtered_urls
    
    def is_product_url(self, url: str) -> bool:
        """Ki·ªÉm tra xem URL c√≥ ph·∫£i l√† URL s·∫£n ph·∫©m kh√¥ng"""
        # URL s·∫£n ph·∫©m th∆∞·ªùng c√≥ format: /category/subcategory/product-name.html
        parts = url.replace(BASE_URL, '').split('/')
        return (len(parts) >= 3 and 
                url.endswith('.html') and
                not any(exclude in url for exclude in ['?', '#', 'page=', 'sort=']))
    
    def crawl_product_detail(self, product_url: str) -> Dict[str, Any]:
        """Crawl th√¥ng tin chi ti·∫øt m·ªôt s·∫£n ph·∫©m"""                                                                                                                                                                                                         
        try:
            response = make_request(product_url, self.session.headers)
            soup = BeautifulSoup(response.content, 'html.parser')

            # Tr√≠ch xu·∫•t th√¥ng tin s·∫£n ph·∫©m theo c·∫•u tr√∫c Long Ch√¢u
            product_data = {
                'url': product_url,
                'name': self.extract_product_name(soup),
                'price': self.extract_price(soup),
                'unit': self.extract_unit(soup),
                'sku': self.extract_sku(soup),
                'rating': self.extract_rating(soup),
                'reviews_count': self.extract_reviews_count(soup),
                'comments_count': self.extract_comments_count(soup),
                'brand': self.extract_brand(soup),
                'official_name': self.extract_official_name(soup),
                'category': self.extract_category(soup),
                'registration_number': self.extract_registration_number(soup),
                'form': self.extract_form(soup),
                'package_size': self.extract_package_size(soup),
                'origin_brand': self.extract_origin_brand(soup),
                'manufacturer': self.extract_manufacturer(soup),
                'country_of_manufacture': self.extract_country_of_manufacture(soup),
                'ingredients': self.extract_ingredients(soup),
                'images': self.extract_images(soup),
                'content': self.extract_content(soup),
                'original_price': self.extract_original_price(soup),
                'discount': self.extract_discount(soup),
                'description': self.extract_description(soup),
                'usage': self.extract_usage(soup),
                'availability': self.extract_availability(soup),
                'crawled_at': datetime.now().isoformat()
            }
            
            return product_data
            
        except Exception as e:
            self.logger.error(f"L·ªói khi crawl s·∫£n ph·∫©m {product_url}: {str(e)}")
            return None
    
    def extract_product_name(self, soup: BeautifulSoup) -> str:
        """Tr√≠ch xu·∫•t t√™n s·∫£n ph·∫©m"""
        # Selector ch√≠nh x√°c t·ª´ Long Ch√¢u
        element = soup.select_one('h1[data-test="product_name"]')
        if element:
            return clean_text(element.get_text())
        
        # Fallback selectors
        selectors = ['h1', '.product-title', '.product-name', '.title']
        for selector in selectors:
            element = soup.select_one(selector)
            if element:
                return clean_text(element.get_text())
        return ""
    
    def extract_price(self, soup: BeautifulSoup) -> float:
        """Tr√≠ch xu·∫•t gi√° s·∫£n ph·∫©m"""
        # Selector ch√≠nh x√°c t·ª´ Long Ch√¢u
        element = soup.select_one('span[data-test="price"]')
        if element:
            price_text = element.get_text()
            if price_text and any(char.isdigit() for char in price_text):
                return format_price(price_text)
        
        # Fallback selectors
        selectors = ['.price', '[class*="price"]', '.cost', '.gia', 'span[class*="price"]', 'div[class*="price"]']
        for selector in selectors:
            element = soup.select_one(selector)
            if element:
                price_text = element.get_text()
                if price_text and any(char.isdigit() for char in price_text):
                    return format_price(price_text)
        return 0.0
    
    def extract_original_price(self, soup: BeautifulSoup) -> float:
        """Tr√≠ch xu·∫•t gi√° g·ªëc"""
        selectors = ['.price-original', '.price-old', '.original-price']
        for selector in selectors:
            element = soup.select_one(selector)
            if element:
                return format_price(element.get_text())
        return 0.0
    
    def extract_discount(self, soup: BeautifulSoup) -> str:
        """Tr√≠ch xu·∫•t th√¥ng tin gi·∫£m gi√°"""
        selectors = ['.discount-percent', '.sale-badge', '.discount']
        for selector in selectors:
            element = soup.select_one(selector)
            if element:
                return clean_text(element.get_text())
        return ""
    
    def extract_description(self, soup: BeautifulSoup) -> str:
        """Tr√≠ch xu·∫•t m√¥ t·∫£ s·∫£n ph·∫©m"""
        selectors = ['.product-description', '.description', '.product-detail']
        for selector in selectors:
            element = soup.select_one(selector)
            if element:
                return clean_text(element.get_text())
        return ""
    
    def extract_ingredients(self, soup: BeautifulSoup) -> str:
        """Tr√≠ch xu·∫•t th√†nh ph·∫ßn"""
        # Tr√≠ch xu·∫•t t·ª´ b·∫£ng th√¥ng tin
        ingredients = self.extract_table_info(soup, "Th√†nh ph·∫ßn")
        if ingredients:
            return ingredients
            
        # Fallback selectors
        selectors = ['.ingredients', '.composition', '.thanh-phan']
        for selector in selectors:
            element = soup.select_one(selector)
            if element:
                return clean_text(element.get_text())
        return ""
    
    def extract_usage(self, soup: BeautifulSoup) -> str:
        """Tr√≠ch xu·∫•t c√°ch s·ª≠ d·ª•ng"""
        selectors = ['.usage', '.cach-dung', '.instructions']
        for selector in selectors:
            element = soup.select_one(selector)
            if element:
                return clean_text(element.get_text())
        return ""
    
    def extract_brand(self, soup: BeautifulSoup) -> str:
        """Tr√≠ch xu·∫•t th∆∞∆°ng hi·ªáu"""
        import json
        import re
        
        # T√¨m trong JSON scripts tr∆∞·ªõc (ngu·ªìn ch√≠nh x√°c nh·∫•t)
        scripts = soup.find_all('script', type='application/json')
        for script in scripts:
            try:
                script_content = script.string
                if script_content:
                    # Th·ª≠ parse JSON
                    try:
                        data = json.loads(script_content)
                        brand_info = self._find_brand_in_json(data)
                        if brand_info:
                            return clean_text(brand_info)
                    except:
                        # N·∫øu kh√¥ng parse ƒë∆∞·ª£c JSON, t√¨m b·∫±ng regex
                        brand_matches = re.findall(r'"brand[^"]*":\s*"([^"]+)"', script_content, re.IGNORECASE)
                        if brand_matches:
                            return clean_text(brand_matches[0])
                        
                        manufacturer_matches = re.findall(r'"manufacturer[^"]*":\s*"([^"]+)"', script_content, re.IGNORECASE)
                        if manufacturer_matches:
                            return clean_text(manufacturer_matches[0])
            except:
                continue
        
        # T√¨m trong structured data (JSON-LD)
        json_ld_scripts = soup.find_all('script', type='application/ld+json')
        for script in json_ld_scripts:
            try:
                data = json.loads(script.string)
                brand_info = self._find_brand_in_json(data)
                if brand_info:
                    return clean_text(brand_info)
            except:
                continue
        
        # T√¨m trong table info (s·ª≠ d·ª•ng extract_table_info)
        brand_from_table = self.extract_table_info(soup, "Th∆∞∆°ng hi·ªáu")
        if brand_from_table:
            return brand_from_table
        
        # T√¨m trong th√¥ng tin s·∫£n ph·∫©m theo c·∫•u tr√∫c Long Ch√¢u
        brand_div = soup.find('div', string=lambda text: text and 'Th∆∞∆°ng hi·ªáu:' in text)
        if brand_div:
            # L·∫•y text sau "Th∆∞∆°ng hi·ªáu: "
            brand_text = brand_div.get_text()
            if 'Th∆∞∆°ng hi·ªáu:' in brand_text:
                return clean_text(brand_text.split('Th∆∞∆°ng hi·ªáu:')[1].strip())
        
        # Fallback selectors
        selectors = ['.brand', '.manufacturer', '.thuong-hieu']
        for selector in selectors:
            element = soup.select_one(selector)
            if element:
                return clean_text(element.get_text())
        return ""
    
    def _find_brand_in_json(self, data, path=""):
        """T√¨m brand trong JSON structure"""
        if isinstance(data, dict):
            for key, value in data.items():
                if 'brand' in key.lower() or 'manufacturer' in key.lower():
                    if isinstance(value, str):
                        return value
                    elif isinstance(value, dict) and 'name' in value:
                        return value['name']
                
                # ƒê·ªá quy t√¨m trong nested objects
                result = self._find_brand_in_json(value, f"{path}.{key}")
                if result:
                    return result
        
        elif isinstance(data, list):
            for i, item in enumerate(data):
                result = self._find_brand_in_json(item, f"{path}[{i}]")
                if result:
                    return result
        
        return None
    
    def extract_category(self, soup: BeautifulSoup) -> str:
        """Tr√≠ch xu·∫•t danh m·ª•c"""
        selectors = ['.breadcrumb', '.category', '.danh-muc']
        for selector in selectors:
            elements = soup.select(f'{selector} a')
            if elements:
                return ' > '.join([clean_text(el.get_text()) for el in elements])
        return ""
    
    def extract_availability(self, soup: BeautifulSoup) -> str:
        """Tr√≠ch xu·∫•t t√¨nh tr·∫°ng c√≤n h√†ng"""
        selectors = ['.availability', '.stock-status', '.tinh-trang']
        for selector in selectors:
            element = soup.select_one(selector)
            if element:
                return clean_text(element.get_text())
        return ""
    
    def extract_images(self, soup: BeautifulSoup) -> List[str]:
        """Tr√≠ch xu·∫•t danh s√°ch ·∫£nh s·∫£n ph·∫©m t·ª´ gallery carousel"""
        images = []
        
        # 1. Tr√≠ch xu·∫•t t·ª´ Swiper carousel (c·∫•u tr√∫c m·ªõi)
        swiper_wrapper = soup.find('div', class_='swiper-wrapper')
        if swiper_wrapper:
            # L·∫•y t·∫•t c·∫£ ·∫£nh trong swiper slides
            swiper_slides = swiper_wrapper.find_all('div', class_='swiper-slide')
            self.logger.info(f"üì∏ Found {len(swiper_slides)} swiper slides")
            
            for i, slide in enumerate(swiper_slides):
                img = slide.find('img', class_='gallery-img')
                if not img:
                    # T√¨m img kh√°c trong slide
                    img = slide.find('img')
                
                if img:
                    # L·∫•y src v√† srcset
                    src = img.get('src', '').strip()
                    srcset = img.get('srcset', '').strip()
                    
                    # Debug log
                    self.logger.debug(f"  Slide {i+1}: src='{src[:50]}...', srcset='{srcset[:50]}...'")
                    
                    # ∆Øu ti√™n l·∫•y URL ch·∫•t l∆∞·ª£ng cao t·ª´ srcset
                    final_src = src
                    if srcset:
                        # L·∫•y URL 2x (ch·∫•t l∆∞·ª£ng cao) t·ª´ srcset
                        srcset_parts = srcset.split(',')
                        for part in srcset_parts:
                            part = part.strip()
                            if '2x' in part:
                                final_src = part.split(' ')[0].strip()
                                break
                        else:
                            # N·∫øu kh√¥ng c√≥ 2x, l·∫•y URL ƒë·∫ßu ti√™n
                            if srcset_parts:
                                final_src = srcset_parts[0].split(' ')[0].strip()
                    
                    # Fallback: n·∫øu kh√¥ng c√≥ src, th·ª≠ data-src
                    if not final_src:
                        final_src = img.get('data-src', '').strip()
                    
                    if final_src and self.is_product_image(final_src):
                        # Chuy·ªÉn ƒë·ªïi sang full size n·∫øu c·∫ßn
                        full_size_src = self.convert_to_full_size_image(final_src)
                        images.append(full_size_src)
                        self.logger.info(f"  ‚úÖ Swiper slide {i+1}: {final_src.split('/')[-1]}")
                    else:
                        if final_src:
                            self.logger.info(f"  ‚ùå Swiper slide {i+1}: {final_src.split('/')[-1]} (filtered)")
                        else:
                            self.logger.info(f"  ‚ùå Swiper slide {i+1}: No src found")
                else:
                    self.logger.info(f"  ‚ùå Swiper slide {i+1}: No img element found")
        
        # 2. Fallback: Tr√≠ch xu·∫•t t·ª´ carousel gallery c≈© (n·∫øu c√≥)
        if not images:
            carousel_gallery = soup.find('div', class_='carousel-gallery-list')
            if carousel_gallery:
                # L·∫•y t·∫•t c·∫£ ·∫£nh trong carousel
                gallery_imgs = carousel_gallery.find_all('img', class_='gallery-img')
                self.logger.info(f"üì∏ Found {len(gallery_imgs)} carousel gallery images (fallback)")
                
                for i, img in enumerate(gallery_imgs):
                    src = img.get('src')
                    srcset = img.get('srcset')
                    
                    # ∆Øu ti√™n l·∫•y URL ch·∫•t l∆∞·ª£ng cao t·ª´ srcset
                    if srcset:
                        # L·∫•y URL 2x (ch·∫•t l∆∞·ª£ng cao) t·ª´ srcset
                        srcset_parts = srcset.split(',')
                        for part in srcset_parts:
                            if '2x' in part:
                                src = part.split(' ')[0].strip()
                                break
                        else:
                            # N·∫øu kh√¥ng c√≥ 2x, l·∫•y URL ƒë·∫ßu ti√™n
                            src = srcset_parts[0].split(' ')[0].strip()
                    
                    if src and self.is_product_image(src):
                        # Chuy·ªÉn ƒë·ªïi sang full size n·∫øu c·∫ßn
                        full_size_src = self.convert_to_full_size_image(src)
                        images.append(full_size_src)
                        self.logger.info(f"  ‚úÖ Carousel {i+1}: {src.split('/')[-1]}")
                    else:
                        if src:
                            self.logger.info(f"  ‚ùå Carousel {i+1}: {src.split('/')[-1]} (filtered)")
        
        # 3. Tr√≠ch xu·∫•t t·ª´ modal gallery (n·∫øu c√≥)
        modal_thumbs = soup.find_all('div', class_='lg-thumb-item')
        if modal_thumbs:
            self.logger.info(f"üñºÔ∏è  Found {len(modal_thumbs)} modal thumbs")
            for i, thumb in enumerate(modal_thumbs):
                img = thumb.find('img')
                if img:
                    src = img.get('src')
                    if src and self.is_product_image(src):
                        # Chuy·ªÉn ƒë·ªïi t·ª´ thumbnail sang full size
                        full_size_src = self.convert_to_full_size_image(src)
                        images.append(full_size_src)
                        self.logger.info(f"  ‚úÖ Modal {i+1}: {src.split('/')[-1]}")
        
        # 4. T√¨m ·∫£nh t·ª´ c√°c script JSON data (n·∫øu c√≥ √≠t ·∫£nh t·ª´ carousel)
        if len(images) < 3:
            self.logger.info("üîç Few carousel images found. Searching JSON scripts...")
            script_tags = soup.find_all('script', type='application/json')
            for script in script_tags:
                try:
                    import re
                    script_content = script.get_text()
                    # T√¨m URLs ·∫£nh s·∫£n ph·∫©m trong JSON (ch·ªâ DSC_ v√† m√£ s·∫£n ph·∫©m)
                    image_urls = re.findall(r'https://cdn\.nhathuoclongchau\.com\.vn/[^"]*(?:DSC_|00\d{6})[^"]*\.(?:jpg|jpeg|png|webp)', script_content, re.IGNORECASE)
                    
                    for url in image_urls:
                        if url not in images and self.is_product_image(url):
                            full_size_url = self.convert_to_full_size_image(url)
                            images.append(full_size_url)
                            self.logger.info(f"  ‚úÖ JSON: {url.split('/')[-1]}")
                            
                            # Gi·ªõi h·∫°n s·ªë ·∫£nh t·ª´ JSON ƒë·ªÉ tr√°nh qu√° nhi·ªÅu
                            if len(images) >= 10:
                                break
                except:
                    continue
        
        # 5. XPath selector fallback (s·ª≠ d·ª•ng lxml n·∫øu c·∫ßn)
        if not images:
            self.logger.info("üîç Using XPath fallback...")
            try:
                from lxml import html
                tree = html.fromstring(str(soup))
                
                # XPath t·ª´ b·∫°n cung c·∫•p (c√≥ th·ªÉ c·∫ßn ƒëi·ªÅu ch·ªânh)
                xpath_selectors = [
                    '//div[@class="swiper-wrapper"]//img[@class="gallery-img"]',
                    '//div[contains(@class,"swiper-slide")]//img',
                    '//img[contains(@class,"gallery-img")]'
                ]
                
                for xpath in xpath_selectors:
                    try:
                        img_elements = tree.xpath(xpath)
                        if img_elements:
                            self.logger.info(f"  Found {len(img_elements)} images with XPath: {xpath}")
                            for img_elem in img_elements:
                                src = img_elem.get('src')
                                srcset = img_elem.get('srcset')
                                
                                # ∆Øu ti√™n srcset 2x
                                if srcset:
                                    srcset_parts = srcset.split(',')
                                    for part in srcset_parts:
                                        if '2x' in part:
                                            src = part.split(' ')[0].strip()
                                            break
                                
                                if src and self.is_product_image(src):
                                    full_size_src = self.convert_to_full_size_image(src)
                                    images.append(full_size_src)
                            
                            if images:
                                break  # N·∫øu t√¨m th·∫•y ·∫£nh, d·ª´ng l·∫°i
                    except:
                        continue
            except ImportError:
                self.logger.warning("lxml not available for XPath, skipping XPath fallback")
            except:
                self.logger.warning("XPath fallback failed")
        
        # 6. Final fallback: T√¨m t·∫•t c·∫£ ·∫£nh s·∫£n ph·∫©m
        if not images:
            self.logger.info("üÜò Using final fallback - searching all images")
            product_imgs = soup.find_all('img')
            for img in product_imgs:
                src = img.get('src') or img.get('data-src')
                if src and self.is_product_image(src):
                    full_size_src = self.convert_to_full_size_image(src)
                    images.append(full_size_src)
        
        # Log k·∫øt qu·∫£
        self.logger.info(f"üìä Image extraction completed: {len(images)} images found")
        
        # Lo·∫°i b·ªè duplicate v√† return
        return list(dict.fromkeys(images))  # Gi·ªØ th·ª© t·ª± v√† lo·∫°i b·ªè duplicate
    
    def extract_content(self, soup: BeautifulSoup) -> str:
        """Tr√≠ch xu·∫•t to√†n b·ªô n·ªôi dung chi ti·∫øt s·∫£n ph·∫©m"""
        try:
            # T√¨m container ch√≠nh ch·ª©a n·ªôi dung s·∫£n ph·∫©m
            content_container = soup.find('div', class_='lc-wrap-content lc-view-full-cont abc')
            
            if content_container:
                # L·∫•y to√†n b·ªô HTML content
                content_html = str(content_container)
                
                # L√†m s·∫°ch v√† format HTML
                content_html = self._clean_content_html(content_html)
                
                self.logger.info(f"‚úÖ Content extracted: {len(content_html)} characters")
                return content_html
            else:
                # Fallback: t√¨m c√°c selector kh√°c
                fallback_selectors = [
                    'div.lc-wrap-content',
                    'div.inner',
                    'div.description',
                    'div[id^="detail-content"]'
                ]
                
                for selector in fallback_selectors:
                    elements = soup.select(selector)
                    if elements:
                        # L·∫•y element ƒë·∫ßu ti√™n ho·∫∑c k·∫øt h·ª£p t·∫•t c·∫£
                        if len(elements) == 1:
                            content_html = str(elements[0])
                        else:
                            # K·∫øt h·ª£p nhi·ªÅu elements
                            content_html = ''.join(str(elem) for elem in elements)
                        
                        content_html = self._clean_content_html(content_html)
                        self.logger.info(f"‚úÖ Content extracted (fallback): {len(content_html)} characters")
                        return content_html
                
                self.logger.warning("‚ùå No content container found")
                return ""
                
        except Exception as e:
            self.logger.error(f"L·ªói khi tr√≠ch xu·∫•t content: {str(e)}")
            return ""
    
    def _clean_content_html(self, html_content: str) -> str:
        """L√†m s·∫°ch v√† format HTML content"""
        try:
            # Parse l·∫°i HTML ƒë·ªÉ l√†m s·∫°ch
            from bs4 import BeautifulSoup
            soup = BeautifulSoup(html_content, 'html.parser')
            
            # Lo·∫°i b·ªè c√°c elements kh√¥ng c·∫ßn thi·∫øt
            unwanted_selectors = [
                'script',
                'style', 
                '.lc-wrap-link',
                '.lc-overlay-detail',
                '[class*="cursor-pointer"]'
            ]
            
            for selector in unwanted_selectors:
                for element in soup.select(selector):
                    element.decompose()
            
            # L√†m s·∫°ch attributes kh√¥ng c·∫ßn thi·∫øt nh∆∞ng gi·ªØ l·∫°i c·∫•u tr√∫c
            for tag in soup.find_all(True):
                # Gi·ªØ l·∫°i m·ªôt s·ªë attributes quan tr·ªçng
                keep_attrs = ['class', 'id', 'href', 'src', 'alt', 'width', 'height']
                attrs_to_remove = []
                
                for attr in tag.attrs:
                    if attr not in keep_attrs:
                        attrs_to_remove.append(attr)
                
                for attr in attrs_to_remove:
                    del tag[attr]
            
            # Tr·∫£ v·ªÅ HTML ƒë√£ ƒë∆∞·ª£c l√†m s·∫°ch
            cleaned_html = str(soup)
            
            # Lo·∫°i b·ªè whitespace th·ª´a
            import re
            cleaned_html = re.sub(r'\s+', ' ', cleaned_html)
            cleaned_html = re.sub(r'>\s+<', '><', cleaned_html)
            
            return cleaned_html.strip()
            
        except Exception as e:
            self.logger.error(f"L·ªói khi l√†m s·∫°ch HTML content: {str(e)}")
            return html_content  # Tr·∫£ v·ªÅ content g·ªëc n·∫øu c√≥ l·ªói
    
    def is_product_image(self, src: str) -> bool:
        """Ki·ªÉm tra xem URL c√≥ ph·∫£i l√† ·∫£nh s·∫£n ph·∫©m kh√¥ng"""
        if not src:
            return False
        
        src_lower = src.lower()
        
        # Lo·∫°i tr·ª´ c√°c ·∫£nh kh√¥ng ph·∫£i s·∫£n ph·∫©m tr∆∞·ªõc
        exclude_patterns = [
            'logo', 'banner', 'icon', 'badge', 'smalls/',
            'facebook', 'zalo', 'download', 'payment',
            'visa', 'master', 'momo', 'napas', 'vnpay',
            'apple_pay', 'amex', 'jcb', 'bo_cong_thuong',
            'legit_', 'dmca_', 'search_', 'menu_',
            'truyen_thong', 'tin_khuyen_mai', 'chuyen_trang',
            'deep_link', 'goc_suc_khoe', 'short_video',
            'cup_', 'benh_', 'thuong_hieu', 'danh_muc',
            'kiem_tra_suc_khoe', 'header_', 'footer_'
        ]
        
        for exclude in exclude_patterns:
            if exclude in src_lower:
                return False
        
        # Ki·ªÉm tra c√°c pattern c·ªßa ·∫£nh s·∫£n ph·∫©m Long Ch√¢u
        # 1. URL t·ª´ CDN ch√≠nh c·ªßa Long Ch√¢u
        if 'cms-prod.s3-sgn09.fptcloud.com' in src_lower:
            # 2. Pattern ·∫£nh s·∫£n ph·∫©m (DSC_ l√† pattern ch·ª•p s·∫£n ph·∫©m)
            if 'dsc_' in src_lower:
                return True
            
            # 3. T√™n s·∫£n ph·∫©m c·ª• th·ªÉ trong URL
            product_name_patterns = [
                'lineabon', 'omexxel', 'calci', 'vitamin', 'nutrimed',
                'nutrigrow', 'osteocare', 'vitabiotics', 'anica',
                'pharma', 'ergopharm', 'nordic', 'excelife'
            ]
            
            for pattern in product_name_patterns:
                if pattern in src_lower:
                    return True
            
            # 4. Ki·ªÉm tra s·ªë s·∫£n ph·∫©m (pattern 00XXXXXX)
            import re
            if re.search(r'00\d{6}', src):
                return True
        
        return False
    
    def convert_to_full_size_image(self, thumbnail_url: str) -> str:
        """Chuy·ªÉn ƒë·ªïi URL thumbnail th√†nh URL full size"""
        if not thumbnail_url:
            return thumbnail_url
        
        # Thay th·∫ø k√≠ch th∆∞·ªõc thumbnail b·∫±ng k√≠ch th∆∞·ªõc l·ªõn h∆°n
        # Pattern: /unsafe/150x0/ -> /unsafe/768x0/ ho·∫∑c /unsafe/1024x0/
        if '/unsafe/150x0/' in thumbnail_url:
            return thumbnail_url.replace('/unsafe/150x0/', '/unsafe/768x0/')
        elif '/unsafe/375x0/' in thumbnail_url:
            return thumbnail_url.replace('/unsafe/375x0/', '/unsafe/768x0/')
        elif '/unsafe/https://' in thumbnail_url:
            # Fix malformed URLs: /unsafe/https://... -> /unsafe/768x0/filters:quality(90)/https://...
            return thumbnail_url.replace('/unsafe/https://', '/unsafe/768x0/filters:quality(90)/https://')
        
        return thumbnail_url
    
    def extract_unit(self, soup: BeautifulSoup) -> str:
        """Tr√≠ch xu·∫•t ƒë∆°n v·ªã t√≠nh"""
        element = soup.select_one('span[data-test="unit"]')
        if element:
            return clean_text(element.get_text())
        return ""
    
    def extract_sku(self, soup: BeautifulSoup) -> str:
        """Tr√≠ch xu·∫•t m√£ s·∫£n ph·∫©m (SKU)"""
        element = soup.select_one('span[data-test-id="sku"]')
        if element:
            return clean_text(element.get_text())
        return ""
    
    def extract_rating(self, soup: BeautifulSoup) -> float:
        """Tr√≠ch xu·∫•t ƒë√°nh gi√° sao"""
        import re
        
        # T√¨m t·∫•t c·∫£ container c√≥ ch·ª©a t·ª´ 'ƒë√°nh gi√°'
        rating_containers = soup.find_all(text=lambda x: x and 'ƒë√°nh gi√°' in x)
        
        for text_node in rating_containers:
            parent = text_node.parent
            if parent:
                full_text = parent.get_text().strip()
                
                # Th·ª≠ c√°c pattern kh√°c nhau ƒë·ªÉ t√¨m rating
                patterns = [
                    r'(\d+(?:\.\d+)?)\s*\(',  # X.X (tr∆∞·ªõc d·∫•u ngo·∫∑c) - pattern t·ªët nh·∫•t
                    r'(\d+(?:\.\d+)?)\s*sao',  # X.X sao
                    r'(\d+(?:\.\d+)?)\s*/',  # X.X/5
                ]
                
                for pattern in patterns:
                    match = re.search(pattern, full_text)
                    if match:
                        try:
                            rating_val = float(match.group(1))
                            # Ki·ªÉm tra xem c√≥ ph·∫£i rating h·ª£p l·ªá kh√¥ng (0-5)
                            if 0 <= rating_val <= 5:
                                return rating_val
                        except:
                            continue
        
        # Fallback: t√¨m trong c√°c container flex items-center
        rating_containers = soup.find_all('div', class_='flex items-center')
        for container in rating_containers:
            rating_text = container.get_text()
            if 'ƒë√°nh gi√°' in rating_text or 'sao' in rating_text:
                # S·ª≠ d·ª•ng pattern tr∆∞·ªõc d·∫•u ngo·∫∑c
                match = re.search(r'(\d+(?:\.\d+)?)\s*\(', rating_text)
                if match:
                    try:
                        rating_val = float(match.group(1))
                        if 0 <= rating_val <= 5:
                            return rating_val
                    except:
                        continue
        
        # Fallback selectors
        selectors = ['.rating', '.stars', '.danh-gia']
        for selector in selectors:
            element = soup.select_one(selector)
            if element:
                text = element.get_text()
                try:
                    return float(''.join(filter(str.isdigit, text.split('/')[0])))
                except:
                    pass
        return 0.0
    
    def extract_reviews_count(self, soup: BeautifulSoup) -> int:
        """Tr√≠ch xu·∫•t s·ªë l∆∞·ª£ng ƒë√°nh gi√°"""
        # T√¨m text ch·ª©a "ƒë√°nh gi√°"
        rating_container = soup.find('div', class_='flex items-center')
        if rating_container:
            text = rating_container.get_text()
            if 'ƒë√°nh gi√°' in text:
                import re
                match = re.search(r'(\d+)\s*ƒë√°nh gi√°', text)
                if match:
                    try:
                        return int(match.group(1))
                    except:
                        pass
        
        # Fallback selectors
        selectors = ['.reviews-count', '.review-count', '.so-danh-gia']
        for selector in selectors:
            element = soup.select_one(selector)
            if element:
                text = element.get_text()
                try:
                    return int(''.join(filter(str.isdigit, text)))
                except:
                    pass
        return 0
    
    def extract_comments_count(self, soup: BeautifulSoup) -> int:
        """Tr√≠ch xu·∫•t s·ªë l∆∞·ª£ng b√¨nh lu·∫≠n"""
        # T√¨m text ch·ª©a "b√¨nh lu·∫≠n"
        rating_container = soup.find('div', class_='flex items-center')
        if rating_container:
            text = rating_container.get_text()
            if 'b√¨nh lu·∫≠n' in text:
                import re
                match = re.search(r'(\d+)\s*b√¨nh lu·∫≠n', text)
                if match:
                    try:
                        return int(match.group(1))
                    except:
                        pass
        return 0
    
    def extract_table_info(self, soup: BeautifulSoup, label: str) -> str:
        """Tr√≠ch xu·∫•t th√¥ng tin t·ª´ b·∫£ng chi ti·∫øt s·∫£n ph·∫©m"""
        # T√¨m trong b·∫£ng content-list
        table = soup.find('table', class_='content-list')
        if table:
            rows = table.find_all('tr', class_='content-container')
            for row in rows:
                cells = row.find_all('td')
                if len(cells) >= 2:
                    label_cell = cells[0]
                    value_cell = cells[1]
                    if label_cell and label in label_cell.get_text():
                        return clean_text(value_cell.get_text())
        return ""
    
    def extract_official_name(self, soup: BeautifulSoup) -> str:
        """Tr√≠ch xu·∫•t t√™n ch√≠nh h√£ng"""
        return self.extract_table_info(soup, "T√™n ch√≠nh h√£ng")
    
    def extract_registration_number(self, soup: BeautifulSoup) -> str:
        """Tr√≠ch xu·∫•t s·ªë ƒëƒÉng k√Ω"""
        return self.extract_table_info(soup, "S·ªë ƒëƒÉng k√Ω")
    
    def extract_form(self, soup: BeautifulSoup) -> str:
        """Tr√≠ch xu·∫•t d·∫°ng b√†o ch·∫ø"""
        return self.extract_table_info(soup, "D·∫°ng b√†o ch·∫ø")
    
    def extract_package_size(self, soup: BeautifulSoup) -> str:
        """Tr√≠ch xu·∫•t quy c√°ch"""
        return self.extract_table_info(soup, "Quy c√°ch")
    
    def extract_origin_brand(self, soup: BeautifulSoup) -> str:
        """Tr√≠ch xu·∫•t xu·∫•t x·ª© th∆∞∆°ng hi·ªáu"""
        return self.extract_table_info(soup, "Xu·∫•t x·ª© th∆∞∆°ng hi·ªáu")
    
    def extract_manufacturer(self, soup: BeautifulSoup) -> str:
        """Tr√≠ch xu·∫•t nh√† s·∫£n xu·∫•t"""
        return self.extract_table_info(soup, "Nh√† s·∫£n xu·∫•t")
    
    def extract_country_of_manufacture(self, soup: BeautifulSoup) -> str:
        """Tr√≠ch xu·∫•t n∆∞·ªõc s·∫£n xu·∫•t"""
        return self.extract_table_info(soup, "N∆∞·ªõc s·∫£n xu·∫•t")
    
    def crawl_category(self, category_url: str, max_products: int = None):
        """Crawl to√†n b·ªô s·∫£n ph·∫©m trong m·ªôt danh m·ª•c"""
        self.logger.info(f"B·∫Øt ƒë·∫ßu crawl danh m·ª•c: {category_url}")
        
        # L∆∞u th√¥ng tin category ƒë·ªÉ d√πng cho t√™n file
        if category_url not in self.current_categories:
            self.current_categories.append(category_url)
        
        # L·∫•y danh s√°ch URL s·∫£n ph·∫©m
        product_urls = self.get_product_urls(category_url)
        
        if max_products:
            product_urls = product_urls[:max_products]
        
        # Crawl t·ª´ng s·∫£n ph·∫©m
        for url in tqdm(product_urls, desc=f"Crawling {category_url}"):
            product_data = self.crawl_product_detail(url)
            if product_data:
                self.products.append(product_data)
            
            random_delay(*RANDOM_DELAY_RANGE)
        
        self.logger.info(f"Ho√†n th√†nh crawl danh m·ª•c {category_url}: {len([p for p in self.products if category_url in p.get('url', '')])} s·∫£n ph·∫©m m·ªõi")
    
    def crawl_subcategories(self, main_category: str, subcategories: list, max_products_per_category: int = None):
        """Crawl t·∫•t c·∫£ subcategories c·ªßa m·ªôt main category"""
        self.logger.info(f"B·∫Øt ƒë·∫ßu crawl main category: {main_category}")
        
        for subcategory in subcategories:
            try:
                category_url = f"{main_category}/{subcategory}"
                self.crawl_category(category_url, max_products_per_category)
            except Exception as e:
                self.logger.error(f"L·ªói khi crawl subcategory {subcategory}: {str(e)}")
                continue
    
    def crawl_vitamin_categories(self, max_products_per_category: int = None):
        """Crawl t·∫•t c·∫£ danh m·ª•c vitamin v√† kho√°ng ch·∫•t"""
        from config.settings import VITAMIN_KHOANG_CHAT
        self.crawl_subcategories("thuc-pham-chuc-nang", VITAMIN_KHOANG_CHAT, max_products_per_category)
    
    def crawl_all_categories(self, max_pages_per_category: int = 5, max_products_per_category: int = None):
        """Crawl t·∫•t c·∫£ danh m·ª•c"""
        for category in CATEGORIES:
            try:
                self.crawl_category(category, max_pages_per_category, max_products_per_category)
            except Exception as e:
                self.logger.error(f"L·ªói khi crawl danh m·ª•c {category}: {str(e)}")
                continue
    
    def save_data(self, format_type: str = 'both'):
        """L∆∞u d·ªØ li·ªáu ƒë√£ crawl"""
        if not self.products:
            self.logger.warning("Kh√¥ng c√≥ d·ªØ li·ªáu ƒë·ªÉ l∆∞u")
            return
        
        timestamp = datetime.now().strftime('%Y%m%d_%H%M%S')
        
        # T·∫°o t√™n file d·ª±a tr√™n categories ƒë√£ crawl
        category_name = self._generate_filename_from_categories()
        
        if format_type in ['json', 'both']:
            json_file = f"data/longchau_products_{category_name}_{timestamp}.json"
            save_to_json(self.products, json_file)
            self.logger.info(f"ƒê√£ l∆∞u {len(self.products)} s·∫£n ph·∫©m v√†o {json_file}")
        
        if format_type in ['csv', 'both']:
            csv_file = f"data/longchau_products_{category_name}_{timestamp}.csv"
            save_to_csv(self.products, csv_file)
            self.logger.info(f"ƒê√£ l∆∞u {len(self.products)} s·∫£n ph·∫©m v√†o {csv_file}")
    
    def _generate_filename_from_categories(self) -> str:
        """T·∫°o t√™n file t·ª´ danh s√°ch categories ƒë√£ crawl"""
        if not self.current_categories:
            return "unknown"
        
        # N·∫øu ch·ªâ c√≥ 1 category, d√πng tr·ª±c ti·∫øp
        if len(self.current_categories) == 1:
            category = self.current_categories[0]
            # Thay th·∫ø k√Ω t·ª± ƒë·∫∑c bi·ªát ƒë·ªÉ t·∫°o t√™n file h·ª£p l·ªá
            return category.replace('/', '_').replace('-', '_')
        
        # N·∫øu c√≥ nhi·ªÅu categories, t√¨m common prefix ho·∫∑c t·∫°o t√™n ng·∫Øn g·ªçn
        categories = self.current_categories
        
        # T√¨m common prefix (main category chung)
        if all('/' in cat for cat in categories):
            main_cats = [cat.split('/')[0] for cat in categories]
            if len(set(main_cats)) == 1:
                # T·∫•t c·∫£ c√πng main category
                main_cat = main_cats[0]
                sub_cats = [cat.split('/')[-1] for cat in categories]
                if len(sub_cats) <= 3:
                    # N·∫øu √≠t subcategories, li·ªát k√™ h·∫øt
                    return f"{main_cat}_{'_'.join(sub_cats)}".replace('-', '_')
                else:
                    # N·∫øu nhi·ªÅu subcategories, d√πng t√™n chung
                    return f"{main_cat}_multiple".replace('-', '_')
        
        # Fallback: d√πng s·ªë l∆∞·ª£ng categories
        return f"multiple_{len(categories)}_categories"
    
    def reset_categories(self):
        """Reset danh s√°ch categories ƒë·ªÉ b·∫Øt ƒë·∫ßu crawl m·ªõi"""
        self.current_categories = []
        self.products = []

if __name__ == "__main__":
    crawler = LongChauCrawler()
    
    # V√≠ d·ª• s·ª≠ d·ª•ng
    try:
        # Crawl danh m·ª•c canxi-vitamin-D c·ª• th·ªÉ
        crawler.crawl_category("thuc-pham-chuc-nang/canxi-vitamin-D", max_products=10)
        
        # Ho·∫∑c crawl t·∫•t c·∫£ danh m·ª•c vitamin
        # crawler.crawl_vitamin_categories(max_products_per_category=5)
        
        # Ho·∫∑c crawl m·ªôt subcategory c·ª• th·ªÉ
        # from config.settings import VITAMIN_KHOANG_CHAT
        # crawler.crawl_subcategories("thuc-pham-chuc-nang", VITAMIN_KHOANG_CHAT[:2], max_products_per_category=5)
        
        # L∆∞u d·ªØ li·ªáu
        crawler.save_data('both')
        
    except KeyboardInterrupt:
        print("\nƒê√£ d·ª´ng crawler. ƒêang l∆∞u d·ªØ li·ªáu...")
        crawler.save_data('both')
    except Exception as e:
        logging.error(f"L·ªói kh√¥ng mong mu·ªën: {str(e)}")
        crawler.save_data('both')
