#!/usr/bin/env python3
"""
Script t√¨m s·∫£n ph·∫©m th·ª±c t·∫ø tr√™n Long Ch√¢u
"""
import requests
from bs4 import BeautifulSoup
import sys
import os
import re

# Th√™m th∆∞ m·ª•c g·ªëc v√†o Python path
sys.path.append(os.path.dirname(__file__))

from config.settings import BASE_URL, DEFAULT_HEADERS

def find_actual_products():
    """T√¨m s·∫£n ph·∫©m th·ª±c t·∫ø"""
    
    # Th·ª≠ c√°c URL c√≥ th·ªÉ ch·ª©a s·∫£n ph·∫©m
    test_urls = [
        f"{BASE_URL}/thuoc/thuoc-khang-sinh-khang-nam",
        f"{BASE_URL}/thuoc/thuoc-tim-mach-and-mau", 
        f"{BASE_URL}/thuoc/thuoc-tieu-hoa-and-gan-mat",
        f"{BASE_URL}/thuc-pham-chuc-nang",
        f"{BASE_URL}/cham-soc-ca-nhan"
    ]
    
    for url in test_urls:
        print(f"\nüîç Testing: {url}")
        
        try:
            response = requests.get(url, headers=DEFAULT_HEADERS, timeout=10)
            soup = BeautifulSoup(response.content, 'html.parser')
            
            print(f"‚úÖ Status: {response.status_code}")
            
            # T√¨m t·∫•t c·∫£ c√°c link
            links = soup.find_all('a', href=True)
            product_links = []
            
            for link in links:
                href = link.get('href')
                text = link.get_text().strip()
                
                # T√¨m link c√≥ pattern gi·ªëng s·∫£n ph·∫©m
                if href and any(pattern in href for pattern in ['/san-pham/', '/product/', '/thuoc/']):
                    if href.startswith('/'):
                        href = BASE_URL + href
                    
                    # Lo·∫°i b·ªè link danh m·ª•c
                    if not any(skip in href for skip in ['/thuoc?', '/thuoc/tra-cuu', '/thuoc/thuoc-']):
                        if len(text) > 5 and len(text) < 100:  # Text h·ª£p l√Ω
                            product_links.append((text, href))
            
            if product_links:
                print(f"üì¶ Found {len(product_links)} potential products:")
                for i, (text, href) in enumerate(product_links[:5]):
                    print(f"   {i+1}. {text[:50]} -> {href}")
                
                # Test trang s·∫£n ph·∫©m ƒë·∫ßu ti√™n
                if product_links:
                    test_product_detail(product_links[0][1])
                    return product_links[0][1]  # Tr·∫£ v·ªÅ URL s·∫£n ph·∫©m ƒë·∫ßu ti√™n
            else:
                print("‚ùå No product links found")
                
        except Exception as e:
            print(f"‚ùå Error: {e}")
    
    return None

def test_product_detail(product_url):
    """Test chi ti·∫øt trang s·∫£n ph·∫©m"""
    print(f"\nüîç Testing product detail: {product_url}")
    
    try:
        response = requests.get(product_url, headers=DEFAULT_HEADERS, timeout=10)
        soup = BeautifulSoup(response.content, 'html.parser')
        
        print(f"‚úÖ Status: {response.status_code}")
        
        # T√¨m th√¥ng tin s·∫£n ph·∫©m v·ªõi nhi·ªÅu selector kh√°c nhau
        selectors_to_test = {
            'title': [
                'h1',
                '.product-title', 
                '.title',
                '[class*="title"]',
                '.name',
                '.product-name'
            ],
            'price': [
                '.price',
                '[class*="price"]',
                '.cost',
                '.gia',
                '[class*="gia"]',
                'span[class*="price"]',
                'div[class*="price"]'
            ],
            'image': [
                '.product-image img',
                '.main-image img', 
                'img[alt*="product"]',
                'img[src*="product"]',
                '.image img',
                'img'
            ]
        }
        
        found_info = {}
        
        for info_type, selectors in selectors_to_test.items():
            print(f"\nüìã Testing {info_type} selectors:")
            for selector in selectors:
                try:
                    elements = soup.select(selector)
                    if elements:
                        element = elements[0]
                        if info_type == 'image':
                            content = element.get('src', element.get('data-src', 'No src'))
                        else:
                            content = element.get_text().strip()
                        
                        if content and len(content) > 0:
                            print(f"   ‚úÖ {selector}: {content[:100]}")
                            if info_type not in found_info:
                                found_info[info_type] = (selector, content)
                            break
                except Exception as e:
                    continue
            
            if info_type not in found_info:
                print(f"   ‚ùå No {info_type} found")
        
        # L∆∞u th√¥ng tin t√¨m ƒë∆∞·ª£c
        if found_info:
            print(f"\n‚úÖ Successfully found selectors:")
            for info_type, (selector, content) in found_info.items():
                print(f"   {info_type}: {selector}")
        
        return found_info
        
    except Exception as e:
        print(f"‚ùå Error: {e}")
        return {}

def search_for_products():
    """T√¨m s·∫£n ph·∫©m qua t√¨m ki·∫øm"""
    search_terms = ['paracetamol', 'vitamin', 'thuoc ho']
    
    for term in search_terms:
        print(f"\nüîç Searching for: {term}")
        search_url = f"{BASE_URL}/tim-kiem?s={term}"
        
        try:
            response = requests.get(search_url, headers=DEFAULT_HEADERS, timeout=10)
            soup = BeautifulSoup(response.content, 'html.parser')
            
            print(f"‚úÖ Status: {response.status_code}")
            
            # T√¨m link s·∫£n ph·∫©m trong k·∫øt qu·∫£ t√¨m ki·∫øm
            links = soup.find_all('a', href=True)
            for link in links:
                href = link.get('href')
                text = link.get_text().strip()
                
                if href and 'san-pham' in href and len(text) > 10:
                    if href.startswith('/'):
                        href = BASE_URL + href
                    print(f"üì¶ Found: {text[:50]} -> {href}")
                    return href
                    
        except Exception as e:
            print(f"‚ùå Error: {e}")
    
    return None

if __name__ == "__main__":
    # Th·ª≠ t√¨m s·∫£n ph·∫©m t·ª´ danh m·ª•c
    product_url = find_actual_products()
    
    # N·∫øu kh√¥ng t√¨m th·∫•y, th·ª≠ t√¨m ki·∫øm
    if not product_url:
        print("\nüîç Trying search...")
        product_url = search_for_products()
    
    if product_url:
        print(f"\n‚úÖ Found working product URL: {product_url}")
    else:
        print("\n‚ùå Could not find any product URLs")
